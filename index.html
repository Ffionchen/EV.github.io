<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>An Embodied Visual Framework for Intelligent Agents with Closed-loop Visual Sensing and Decision-making</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

</head>
<body>

<div class="page-container">
<header class="main-header">
    <h1>Embodied Visual Framework for Closed-loop Perception-Decision Making in Intelligent Agents</h1>
    
    <p class="authors">
        Jiahong Zhang · Zifan Chen · Haijian Qin · Wangtian Shen · Man Yao · 
        Jinning Zhao · Tianxiang Hu · Yu Qiao · Wei Wang · Jinhu Lü · 
        Ziyang Meng* · Bo Xu · Guoqi Li*
    </p>
    
    <p>
        <a href="https://arxiv.org/abs/2311.03357" target="_blank">
            <img src="asserts/images/link1.svg" alt="Paper" style="height: 35px;">
        </a> 
        <a href="https://github.com/xgxvisnav/xgxvisnav.github.io" target="_blank">
            <img src="asserts/images/link2.svg" alt="Code" style="height: 35px;">
        </a>
    </p>
    
    <p class="corresponding">
        * Corresponding authors: ziyangmeng@mail.tsinghua.edu.cn; guoqi.li@ia.ac.cn
    </p>

</header>

<section class="abstract">
    <h2>Abstract</h2>
    <div class="abstract-content">
        <p>
            <!-- 摘要内容 -->
            Current intelligent agents largely rely on disembodied visual sensing: observations are treated as fixed inputs that cannot be explicitly regulated to meet task demands, limiting adaptive perception and decision-making in dynamic environments.
In contrast, biological vision is inherently embodied, supported by dedicated sensory organs that enable precise, purposeful modulation of perceptual input.
This discrepancy highlights a critical structural deficiency in today's agents, where the lack of a controllable visual perceptual substrate fundamentally compromises their adaptability to dynamic or novel scenarios.
To bridge this gap, we propose an embodied visual framework that formalizes visual embodiment via an algorithmic “eye” serving as a visual “body” for agents. 
Instead of consuming a static input stream, the algorithmic “eye” actively regulates visual sensing, and is jointly optimized with representation learning and task-driven decision-making to form a closed perception-decision loop.
Across diverse tasks, agents equipped with embodied vision achieve higher accuracy, robustness, and generalization.
In static recognition, our approach improves accuracy and enhances robustness to ImageNet corruptions by 8.1%. In interactive decision-making on the Arcade Learning Environment, it substantially improves task performance, increasing the Amidar score by approximately 100 points. In navigation, robustness to visual corruptions improves by 8.8%, and real-world navigation achieves nearly a 2× increase in success rate compared with baseline methods.
Beyond performance, the learned visual representations exhibit closer correspondence to the human visual cortex, while the learned gaze patterns provide interpretable insights into the agent's decision-making.
Collectively, these findings suggest that making visual perception controllable and embodied is a fundamental design principle for building accurate, robust, and interpretable intelligent agents.
        </p>
    </div>
</section>

<section class="Embodied Visual Framework ">
    <div class="section-title two-column">
        <h2 class="left-title">Disembodied Vision v.s. Embodied Vision</h2>
        <h2 class="right-title">Embodied Visual Framework</h2>
    </div>
    <div class="image-grid large two-column">
        <img src="asserts/images/fig1.png" alt="">
        <img src="asserts/images/fig2.png" alt="">
    </div>
</section>

<section class="results">
    <!-- 分类任务 -->
    <h2>Classification Task Performance</h2>
    <p>Computational cost analysis of ResNet18-EV under different numbers of eye-movement steps (EM) and
        post-retina resolutions. Restricting iterative processing to the stem enables substantial FLOPs reduction while preserving
        competitive accuracy, supporting an efficient accuracy-computation trade-off.</p>
    <div class="image-grid">
        <img src="asserts/images/fig1-1.png" alt="">
    </div>    
    
    <div class="image-grid four-column">
        <video autoplay muted loop playsinline>
            <source src="asserts/video-class/4-mask.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
            <source src="asserts/video-class/4.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
            <source src="asserts/video-class/10-mask.mp4" type="video/mp4">
        </video>
        <video autoplay muted loop playsinline>
            <source src="asserts/video-class/10.mp4" type="video/mp4">
        </video>
    </div>
    
        <h2>Simulation Results</h2>
    <p>
        Simulation Object Goal Task Performance of XGX (baseline) and our proposed EV-XGX (our method). 
        Metrics include SR, SPL, and softSPL (higher is better).
    </p>
    <div class="image-grid large">
        <img src="asserts/images/result1.png" alt="">
    </div>

    <h3>Example Simulation Rollout</h3>
    <p>Below are examples of rollouts collected in the simulation. </p>
    <p>Example 1  Goal: Chair</p>
    <div class="image-grid">
        <video autoplay muted loop controls width="800">
            <source src="asserts/video-class/episode=0chair.mp4" type="video/mp4">
        </video>
    </div>
    <p>Example 2  Goal: Bed</p>
    <div class="image-grid">
        <video autoplay muted loop controls width="800">
            <source src="asserts/video-class/episode=1bed.mp4" type="video/mp4">
        </video>
    </div>
</section>

<section class="robot">
    <h2>Real World Results</h2>
    <h3>Robot Morphology</h3>
    <p>
        Real-world deployment of the proposed embodied visual navigation system on the Autolabor Pro1 mobile robot.
    </p>
    <div class="image-grid">
        <img src="asserts/images/robot.png" alt="">
    </div>

    <h3>Real-World Object-Goal Navigation Results</h3>
    <p>
        Real-World Object-Goal Navigation Results.
    </p>
    <div class="image-grid">
        <img src="asserts/images/result2.png" alt="">
    </div>

</section>
<section class="media">
    <h3>Example Real-World Rollout</h3>
    <p>Below are examples of rollouts collected in the real-world. </p>
    <p>Example 1  Goal: TV</p>
    <video controls width="800">
        <source src="asserts/videos_5x_silent/VID_20251116_140630.mp4" type="video/mp4">
    </video>
    <p>Example 2  Goal: Chair</p>
    <video controls width="800">
        <source src="asserts/videos_5x_silent/VID_20251116_155626.mp4" type="video/mp4">
    </video>
</section>

<footer class="main-footer">
</footer>
</div>
</body>
</html>
